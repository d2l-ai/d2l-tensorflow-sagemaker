{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# GPUs\n",
    ":label:`sec_use_gpu`\n",
    "\n",
    "Em :numref:`tab_intro_decade`, discutimos o rápido crescimento\n",
    "de computação nas últimas duas décadas.\n",
    "Em suma, o desempenho da GPU aumentou\n",
    "por um fator de 1000 a cada década desde 2000.\n",
    "Isso oferece ótimas oportunidades, mas também sugere\n",
    "uma necessidade significativa de fornecer tal desempenho.\n",
    "\n",
    "Nesta seção, começamos a discutir como aproveitar\n",
    "este desempenho computacional para sua pesquisa.\n",
    "Primeiro usando GPUs únicas e, posteriormente,\n",
    "como usar várias GPUs e vários servidores (com várias GPUs).\n",
    "\n",
    "Especificamente, discutiremos como\n",
    "para usar uma única GPU NVIDIA para cálculos.\n",
    "Primeiro, certifique-se de ter pelo menos uma GPU NVIDIA instalada.\n",
    "Em seguida, baixe o [NVIDIA driver e CUDA](https://developer.nvidia.com/cuda-downloads).\n",
    "e siga as instruções para definir o caminho apropriado.\n",
    "Assim que esses preparativos forem concluídos,\n",
    "o comando `nvidia-smi` pode ser usado\n",
    "para ver as informações da placa gráfica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 1,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec 11 07:57:09 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:1B.0 Off |                    0 |\r\n",
      "| N/A   40C    P0    50W / 300W |     11MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   1  Tesla V100-SXM2...  Off  | 00000000:00:1C.0 Off |                    0 |\r\n",
      "| N/A   36C    P0    50W / 300W |    584MiB / 16130MiB |      5%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla V100-SXM2...  Off  | 00000000:00:1D.0 Off |                    0 |\r\n",
      "| N/A   73C    P0   276W / 300W |   8892MiB / 16130MiB |     98%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   51C    P0    48W / 300W |     11MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    1     80341      C   ...conda3/envs/d2l-pt-release-0/bin/python   573MiB |\r\n",
      "|    2     90676      C   ...conda3/envs/d2l-pt-release-0/bin/python  8881MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "Para executar os programas desta seção,\n",
    "você precisa de pelo menos duas GPUs.\n",
    "Observe que isso pode ser extravagante para a maioria dos computadores desktop\n",
    "mas está facilmente disponível na nuvem, por exemplo,\n",
    "usando as instâncias multi-GPU do AWS EC2.\n",
    "Quase todas as outras seções * não * requerem várias GPUs.\n",
    "Em vez disso, isso é simplesmente para ilustrar\n",
    "como os dados fluem entre diferentes dispositivos.\n",
    "\n",
    "## Dispositivos Computacionais\n",
    "\n",
    "Podemos especificar dispositivos, como CPUs e GPUs,\n",
    "para armazenamento e cálculo.\n",
    "Por padrão, os tensores são criados na memória principal\n",
    "e, em seguida, use a CPU para calculá-lo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.eager.context._EagerDeviceContext at 0x7ff644556550>,\n",
       " <tensorflow.python.eager.context._EagerDeviceContext at 0x7ff644556ee0>,\n",
       " <tensorflow.python.eager.context._EagerDeviceContext at 0x7ff6445566d0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.device('/CPU:0'), tf.device('/GPU:0'), tf.device('/GPU:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "Podemos consultar o número de GPUs disponíveis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 13,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "Agora definimos duas funções convenientes que nos permitem\n",
    "para executar o código mesmo que as GPUs solicitadas não existam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.eager.context._EagerDeviceContext at 0x7ff59d9e6b20>,\n",
       " <tensorflow.python.eager.context._EagerDeviceContext at 0x7ff59004d040>,\n",
       " [<tensorflow.python.eager.context._EagerDeviceContext at 0x7ff59004d070>,\n",
       "  <tensorflow.python.eager.context._EagerDeviceContext at 0x7ff59004d1c0>])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if len(tf.config.experimental.list_physical_devices('GPU')) >= i + 1:\n",
    "        return tf.device(f'/GPU:{i}')\n",
    "    return tf.device('/CPU:0')\n",
    "\n",
    "def try_all_gpus():  #@save\n",
    "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n",
    "    num_gpus = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "    devices = [tf.device(f'/GPU:{i}') for i in range(num_gpus)]\n",
    "    return devices if devices else [tf.device('/CPU:0')]\n",
    "\n",
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "## Tensores e GPUs\n",
    "\n",
    "Por padrão, tensores são criados na CPU.\n",
    "Podemos consultar o dispositivo onde o tensor está localizado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([1, 2, 3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "É importante notar que sempre que quisermos\n",
    "para operar em vários termos,\n",
    "eles precisam estar no mesmo dispositivo.\n",
    "Por exemplo, se somarmos dois tensores,\n",
    "precisamos ter certeza de que ambos os argumentos\n",
    "estão no mesmo dispositivo --- caso contrário, a estrutura\n",
    "não saberia onde armazenar o resultado\n",
    "ou mesmo como decidir onde realizar o cálculo.\n",
    "\n",
    "### Armazenamento na GPU\n",
    "\n",
    "Existem várias maneiras de armazenar um tensor na GPU.\n",
    "Por exemplo, podemos especificar um dispositivo de armazenamento ao criar um tensor.\n",
    "A seguir, criamos a variável tensorial `X` no primeiro `gpu`.\n",
    "O tensor criado em uma GPU consome apenas a memória desta GPU.\n",
    "Podemos usar o comando `nvidia-smi` para ver o uso de memória da GPU.\n",
    "Em geral, precisamos ter certeza de não criar dados que excedam o limite de memória da GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 25,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with try_gpu():\n",
    "    X = tf.ones((2, 3))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "Supondo que você tenha pelo menos duas GPUs, o código a seguir criará um tensor aleatório na segunda GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 29,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.9953705 , 0.2957828 , 0.80567956],\n",
       "       [0.11053193, 0.7893543 , 0.5817497 ]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with try_gpu(1):\n",
    "    Y = tf.random.uniform((2, 3))\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "### Copiando\n",
    "\n",
    "Se quisermos calcular `X + Y`,\n",
    "precisamos decidir onde realizar esta operação.\n",
    "Por exemplo, como mostrado em :numref:`fig_copyto`,\n",
    "podemos transferir `X` para a segunda GPU\n",
    "e realizar a operação lá.\n",
    "*Não* simplesmente adicione `X` e` Y`,\n",
    "pois isso resultará em uma exceção.\n",
    "O mecanismo de tempo de execução não saberia o que fazer:\n",
    "ele não consegue encontrar dados no mesmo dispositivo e falha.\n",
    "Já que `Y` vive na segunda GPU,\n",
    "precisamos mover `X` para lá antes de podermos adicionar os dois.\n",
    "\n",
    "![Copiar dados para realizar uma operação no mesmo dispositivo.](../img/copyto.svg)\n",
    ":label:`fig_copyto`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 33,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with try_gpu(1):\n",
    "    Z = X\n",
    "print(X)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "Agora que os dados estão na mesma GPU\n",
    "(ambos são `Z` e` Y`),\n",
    "podemos somá-los.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 35,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1.9953705, 1.2957828, 1.8056796],\n",
       "       [1.1105319, 1.7893543, 1.5817497]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y + Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 38,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "Imagine que sua variável `Z` já esteja em sua segunda GPU.\n",
    "O que acontece se ainda chamarmos `Z2 = Z` no mesmo escopo de dispositivo?\n",
    "Ele retornará `Z` em vez de fazer uma cópia e alocar nova memória.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 41,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with try_gpu(1):\n",
    "    Z2 = Z\n",
    "Z2 is Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 42
   },
   "source": [
    "### Informações extra\n",
    "\n",
    "As pessoas usam GPUs para fazer aprendizado de máquina\n",
    "porque eles esperam que ela seja rápida.\n",
    "Mas a transferência de variáveis entre dispositivos é lenta.\n",
    "Então, queremos que você tenha 100% de certeza\n",
    "que você deseja fazer algo lento antes de deixá-lo fazer.\n",
    "Se a estrutura de *Deep Learning* apenas fizesse a cópia automaticamente\n",
    "sem bater, então você pode não perceber\n",
    "que você escreveu algum código lento.\n",
    "\n",
    "Além disso, a transferência de dados entre dispositivos (CPU, GPUs e outras máquinas)\n",
    "é algo muito mais lento do que a computação.\n",
    "Também torna a paralelização muito mais difícil,\n",
    "já que temos que esperar que os dados sejam enviados (ou melhor, para serem recebidos)\n",
    "antes de prosseguirmos com mais operações.\n",
    "É por isso que as operações de cópia devem ser realizadas com muito cuidado.\n",
    "Como regra geral, muitas pequenas operações\n",
    "são muito piores do que uma grande operação.\n",
    "Além disso, várias operações ao mesmo tempo\n",
    "são muito melhores do que muitas operações simples intercaladas no código\n",
    "a menos que você saiba o que está fazendo.\n",
    "Este é o caso, uma vez que tais operações podem bloquear se um dispositivo\n",
    "tem que esperar pelo outro antes de fazer outra coisa.\n",
    "É um pouco como pedir seu café em uma fila\n",
    "em vez de pré-encomendá-lo por telefone\n",
    "e descobrir que ele está pronto quando você estiver.\n",
    "\n",
    "Por último, quando imprimimos tensores ou convertemos tensores para o formato NumPy,\n",
    "se os dados não estiverem na memória principal,\n",
    "o framework irá copiá-lo para a memória principal primeiro,\n",
    "resultando em sobrecarga de transmissão adicional.\n",
    "Pior ainda, agora está sujeito ao temido bloqueio de intérprete global\n",
    "isso faz tudo esperar que o Python seja concluído.\n",
    "\n",
    "\n",
    "## Redes Neurais e GPUs\n",
    "\n",
    "Da mesma forma, um modelo de rede neural pode especificar dispositivos.\n",
    "O código a seguir coloca os parâmetros do modelo na GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 45,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    net = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 46
   },
   "source": [
    "Veremos muitos mais exemplos de\n",
    "como executar modelos em GPUs nos capítulos seguintes,\n",
    "simplesmente porque eles se tornarão um pouco mais intensivos em termos de computação.\n",
    "\n",
    "Quando a entrada é um tensor na GPU, o modelo calculará o resultado na mesma GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "origin_pos": 47,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[-0.97097045],\n",
       "       [-0.97097045]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 48
   },
   "source": [
    "Vamos confirmar se os parâmetros do modelo estão armazenados na mesma GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "origin_pos": 51,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[0].weights[0].device, net.layers[0].weights[1].device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 52
   },
   "source": [
    "Resumindo, contanto que todos os dados e parâmetros estejam no mesmo dispositivo, podemos aprender modelos com eficiência. Nos próximos capítulos, veremos vários desses exemplos.\n",
    "\n",
    "## Sumário\n",
    "\n",
    "* Podemos especificar dispositivos para armazenamento e cálculo, como CPU ou GPU.\n",
    "   Por padrão, os dados são criados na memória principal\n",
    "   e então usa-se a CPU para cálculos.\n",
    "* A estrutura de *Deep Learning* requer todos os dados de entrada para cálculo\n",
    "   estar no mesmo dispositivo,\n",
    "   seja CPU ou a mesma GPU.\n",
    "* Você pode perder um desempenho significativo movendo dados sem cuidado.\n",
    "   Um erro típico é o seguinte: calcular a perda\n",
    "   para cada minibatch na GPU e relatando de volta\n",
    "   para o usuário na linha de comando (ou registrando-o em um NumPy `ndarray`)\n",
    "   irá disparar um bloqueio global do interpretador que paralisa todas as GPUs.\n",
    "   É muito melhor alocar memória\n",
    "   para registrar dentro da GPU e apenas mover registros maiores.\n",
    "\n",
    "## Exercícios\n",
    "\n",
    "1. Tente uma tarefa de computação maior, como a multiplicação de grandes matrizes,\n",
    "    e veja a diferença de velocidade entre a CPU e a GPU.\n",
    "    Que tal uma tarefa com uma pequena quantidade de cálculos?\n",
    "1. Como devemos ler e escrever os parâmetros do modelo na GPU?\n",
    "1. Meça o tempo que leva para calcular 1000\n",
    "    multiplicações matriz-matriz de $100 \\times 100$ matrizes\n",
    "    e registrar a norma de Frobenius da matriz de saída, um resultado de cada vez\n",
    "    vs. manter um registro na GPU e transferir apenas o resultado final.\n",
    "1. Meça quanto tempo leva para realizar duas multiplicações matriz-matriz\n",
    "    em duas GPUs ao mesmo tempo vs. em sequência\n",
    "    em uma GPU. Dica: você deve ver uma escala quase linear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 55,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "[Discussão](https://discuss.d2l.ai/t/270)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 56
   },
   "source": [
    "<!--stackedit_data:\n",
    "eyJoaXN0b3J5IjpbMTM1NzUzMTk4OCwtMTA5NzI3MTAzNiw4MD\n",
    "c2MDQzNTksLTY3MjIyODU4NF19\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}