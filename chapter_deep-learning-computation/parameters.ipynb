{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Gerenciamento de Parâmetros\n",
    "\n",
    "Depois de escolher uma arquitetura\n",
    "e definir nossos hiperparâmetros,\n",
    "passamos para o ciclo de treinamento,\n",
    "onde nosso objetivo é encontrar valores de parâmetro\n",
    "que minimizam nossa função de perda.\n",
    "Após o treinamento, precisaremos desses parâmetros\n",
    "para fazer previsões futuras.\n",
    "Além disso, às vezes desejamos\n",
    "para extrair os parâmetros\n",
    "seja para reutilizá-los em algum outro contexto,\n",
    "para salvar nosso modelo em disco para que\n",
    "pode ser executado em outro *software*,\n",
    "ou para exame na esperança de\n",
    "ganhar compreensão científica.\n",
    "\n",
    "Na maioria das vezes, seremos capazes de\n",
    "ignorar os detalhes essenciais\n",
    "de como os parâmetros são declarados\n",
    "e manipulado, contando com estruturas de *Deep Learning*\n",
    "para fazer o trabalho pesado.\n",
    "No entanto, quando nos afastamos de\n",
    "arquiteturas empilhadas com camadas padrão,\n",
    "às vezes precisaremos \n",
    "declarar e manipular parâmetros.\n",
    "Nesta seção, cobrimos o seguinte:\n",
    "\n",
    "* Parâmetros de acesso para depuração, diagnóstico e visualizações.\n",
    "* Inicialização de parâmetros.\n",
    "* Parâmetros de compartilhamento em diferentes componentes do modelo.\n",
    "\n",
    "Começamos nos concentrando em um MLP com uma camada oculta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[-0.08388045],\n",
       "       [-0.040921  ]], dtype=float32)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "X = tf.random.uniform((2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## Acesso a Parâmetros\n",
    "\n",
    "Vamos começar explicando como acessar os parâmetros\n",
    "dos modelos que você já conhece.\n",
    "Quando um modelo é definido por meio da classe `Sequential`,\n",
    "podemos primeiro acessar qualquer camada indexando\n",
    "no modelo como se fosse uma lista.\n",
    "Os parâmetros de cada camada são convenientemente\n",
    "localizado em seu atributo.\n",
    "Podemos inspecionar os parâmetros da segunda camada totalmente conectada da seguinte maneira.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_1/kernel:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[-0.8849803 ],\n",
      "       [ 0.28838134],\n",
      "       [ 0.8427515 ],\n",
      "       [ 0.29679787]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(net.layers[2].weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "A saída nos diz algumas coisas importantes.\n",
    "Primeiro, esta camada totalmente conectada\n",
    "contém dois parâmetros,\n",
    "correspondendo aos\n",
    "pesos e vieses, respectivamente.\n",
    "Ambos são armazenados como *floats* de precisão simples (float32).\n",
    "Observe que os nomes dos parâmetros\n",
    "nos permitem identificar de forma única\n",
    "parâmetros de cada camada,\n",
    "mesmo em uma rede contendo centenas de camadas.\n",
    "\n",
    "\n",
    "### Parâmetros Direcionados\n",
    "\n",
    "Observe que cada parâmetro é representado\n",
    "como uma instância da classe de parâmetro.\n",
    "Para fazer algo útil com os parâmetros,\n",
    "primeiro precisamos acessar os valores numéricos subjacentes.\n",
    "Existem várias maneiras de fazer isso.\n",
    "Alguns são mais simples, enquanto outros são mais gerais.\n",
    "O código a seguir extrai o viés\n",
    "da segunda camada de rede neural, que retorna uma instância de classe de parâmetro, e\n",
    "acessa posteriormente o valor desse parâmetro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "<tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>\n",
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(type(net.layers[2].weights[1]))\n",
    "print(net.layers[2].weights[1])\n",
    "print(tf.convert_to_tensor(net.layers[2].weights[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "### Todos os Parâmetros de Uma Vez\n",
    "\n",
    "Quando precisamos realizar operações em todos os parâmetros,\n",
    "acessá-los um por um pode se tornar tedioso.\n",
    "A situação pode ficar especialmente complicada\n",
    "quando trabalhamos com blocos mais complexos (por exemplo, blocos aninhados),\n",
    "uma vez que precisaríamos voltar recursivamente\n",
    "através de toda a árvore para extrair\n",
    "parâmetros de cada sub-bloco. Abaixo, demonstramos como acessar os parâmetros da primeira camada totalmente conectada versus acessar todas as camadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 18,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
      "array([[-0.00696713,  0.06294048, -0.34609908,  0.28365868],\n",
      "       [ 0.05357468, -0.3715149 , -0.21233797,  0.5667568 ],\n",
      "       [ 0.75887805, -0.24776524,  0.58249944,  0.15801603],\n",
      "       [-0.309555  , -0.46662605,  0.3352285 , -0.7561607 ]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n",
      "[array([[-0.00696713,  0.06294048, -0.34609908,  0.28365868],\n",
      "       [ 0.05357468, -0.3715149 , -0.21233797,  0.5667568 ],\n",
      "       [ 0.75887805, -0.24776524,  0.58249944,  0.15801603],\n",
      "       [-0.309555  , -0.46662605,  0.3352285 , -0.7561607 ]],\n",
      "      dtype=float32), array([0., 0., 0., 0.], dtype=float32), array([[-0.8849803 ],\n",
      "       [ 0.28838134],\n",
      "       [ 0.8427515 ],\n",
      "       [ 0.29679787]], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(net.layers[1].weights)\n",
    "print(net.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "Isso nos fornece outra maneira de acessar os parâmetros da rede como segue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_weights()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "### Coletando Parâmetros de Blocos Aninhados\n",
    "\n",
    "Vamos ver como funcionam as convenções de nomenclatura de parâmetros\n",
    "se aninharmos vários blocos uns dentro dos outros.\n",
    "Para isso, primeiro definimos uma função que produz blocos\n",
    "(uma fábrica de blocos, por assim dizer) e então\n",
    "combine-os dentro de blocos ainda maiores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 26,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[-0.04170988],\n",
       "       [-0.21115464]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1(name):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(4, activation=tf.nn.relu)],\n",
    "        name=name)\n",
    "\n",
    "def block2():\n",
    "    net = tf.keras.Sequential()\n",
    "    for i in range(4):\n",
    "        # Nested here\n",
    "        net.add(block1(name=f'block-{i}'))\n",
    "    return net\n",
    "\n",
    "rgnet = tf.keras.Sequential()\n",
    "rgnet.add(block2())\n",
    "rgnet.add(tf.keras.layers.Dense(1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 27
   },
   "source": [
    "Agora que projetamos a rede,\n",
    "vamos ver como está organizado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_2 (Sequential)    (2, 4)                    80        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (2, 1)                    5         \n",
      "=================================================================\n",
      "Total params: 85\n",
      "Trainable params: 85\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(rgnet.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 31
   },
   "source": [
    "Uma vez que as camadas são aninhadas hierarquicamente,\n",
    "também podemos acessá-los como se\n",
    "indexação por meio de listas aninhadas.\n",
    "Por exemplo, podemos acessar o primeiro bloco principal,\n",
    "dentro dele o segundo sub-bloco,\n",
    "e dentro disso o viés da primeira camada,\n",
    "com o seguinte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 34,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'sequential_2/block-1/dense_3/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet.layers[0].layers[1].layers[1].weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "## Inicialização de Parâmetros\n",
    "\n",
    "Agora que sabemos como acessar os parâmetros,\n",
    "vamos ver como inicializá-los corretamente.\n",
    "Discutimos a necessidade de inicialização adequada em :numref:`sec_numerical_stability`.\n",
    "A estrutura de *Deep Learning* fornece inicializações aleatórias padrão para suas camadas.\n",
    "No entanto, muitas vezes queremos inicializar nossos pesos\n",
    "de acordo com vários outros protocolos. A estrutura fornece mais comumente\n",
    "protocolos usados e também permite criar um inicializador personalizado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 38,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "Por padrão, Keras inicializa matrizes de ponderação uniformemente, tirando de um intervalo que é calculado de acordo com a dimensão de entrada e saída, e os parâmetros de polarização são todos definidos como zero.\n",
    "O TensorFlow oferece uma variedade de métodos de inicialização no módulo raiz e no módulo `keras.initializers`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 39
   },
   "source": [
    "### Inicialização *Built-in* \n",
    "\n",
    "Vamos começar chamando inicializadores integrados.\n",
    "O código abaixo inicializa todos os parâmetros de peso\n",
    "como variáveis aleatórias gaussianas\n",
    "com desvio padrão de 0,01, enquanto os parâmetros de polarização são zerados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 42,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_7/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       " array([[-0.00880642,  0.00698964, -0.00339042, -0.01238017],\n",
       "        [-0.00936008,  0.00712868,  0.0120426 ,  0.01030472],\n",
       "        [-0.00444559,  0.01566276, -0.00378464, -0.00585882],\n",
       "        [-0.00496555,  0.00707446, -0.0006112 ,  0.00711826]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_7/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4, activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.01),\n",
    "        bias_initializer=tf.zeros_initializer()),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "\n",
    "net(X)\n",
    "net.weights[0], net.weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 43
   },
   "source": [
    "Também podemos inicializar todos os parâmetros\n",
    "a um determinado valor constante (digamos, 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 46,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_9/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Variable 'dense_9/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4, activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.keras.initializers.Constant(1),\n",
    "        bias_initializer=tf.zeros_initializer()),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "net.weights[0], net.weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 47
   },
   "source": [
    "Também podemos aplicar inicializadores diferentes para certos blocos.\n",
    "Por exemplo, abaixo inicializamos a primeira camada\n",
    "com o inicializador Xavier\n",
    "e inicializar a segunda camada\n",
    "para um valor constante de 42.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 50,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'dense_11/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
      "array([[-0.2608738 , -0.22718716,  0.11381751, -0.08536631],\n",
      "       [-0.32224733,  0.3555506 ,  0.52300423, -0.6358117 ],\n",
      "       [ 0.52879506,  0.32641047, -0.37077838, -0.72854525],\n",
      "       [-0.29767555, -0.3362043 , -0.71178615, -0.1830396 ]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'dense_12/kernel:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4,\n",
    "        activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.keras.initializers.GlorotUniform()),\n",
    "    tf.keras.layers.Dense(\n",
    "        1, kernel_initializer=tf.keras.initializers.Constant(1)),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "print(net.layers[1].weights[0])\n",
    "print(net.layers[2].weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 51
   },
   "source": [
    "### Inicialização Customizada\n",
    "\n",
    "Às vezes, os métodos de inicialização de que precisamos\n",
    "não são fornecidos pela estrutura de *Deep Learning*.\n",
    "No exemplo abaixo, definimos um inicializador\n",
    "para qualquer parâmetro de peso $w$ usando a seguinte distribuição estranha:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ with probability } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ with probability } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ with probability } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 54,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "Aqui nós definimos uma subclasse de `Initializer` e implementamos o`__call__`\n",
    "função que retorna um tensor desejado de acordo com a forma e o tipo de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "origin_pos": 57,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'dense_13/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
      "array([[ 5.46612  , -0.       ,  8.619307 ,  0.       ],\n",
      "       [ 7.9447002,  9.824312 ,  6.20899  , -6.143732 ],\n",
      "       [ 8.9025135, -6.1618114, -6.355505 , -6.9544387],\n",
      "       [ 0.       , -0.       ,  5.6121492, -5.8810377]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "class MyInit(tf.keras.initializers.Initializer):\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        data=tf.random.uniform(shape, -10, 10, dtype=dtype)\n",
    "        factor=(tf.abs(data) >= 5)\n",
    "        factor=tf.cast(factor, tf.float32)\n",
    "        return data * factor\n",
    "\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4,\n",
    "        activation=tf.nn.relu,\n",
    "        kernel_initializer=MyInit()),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "print(net.layers[1].weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 58
   },
   "source": [
    "Observe que sempre temos a opção\n",
    "de definir parâmetros diretamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "origin_pos": 61,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_13/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       "array([[42.       ,  1.       ,  9.619307 ,  1.       ],\n",
       "       [ 8.9447   , 10.824312 ,  7.20899  , -5.143732 ],\n",
       "       [ 9.9025135, -5.1618114, -5.355505 , -5.9544387],\n",
       "       [ 1.       ,  1.       ,  6.6121492, -4.8810377]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[1].weights[0][:].assign(net.layers[1].weights[0] + 1)\n",
    "net.layers[1].weights[0][0, 0].assign(42)\n",
    "net.layers[1].weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 63
   },
   "source": [
    "## Parâmetros *Tied*\n",
    "\n",
    "\n",
    "Frequentemente, queremos compartilhar parâmetros em várias camadas.\n",
    "Vamos ver como fazer isso com elegância.\n",
    "A seguir, alocamos uma camada densa\n",
    "e usar seus parâmetros especificamente\n",
    "para definir os de outra camada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "origin_pos": 66,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# tf.keras behaves a bit differently. It removes the duplicate layer\n",
    "# automatically\n",
    "shared = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    shared,\n",
    "    shared,\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "# Checando se os parâmetros são diferentes\n",
    "print(len(net.layers) == 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 68
   },
   "source": [
    "## Sumário\n",
    "\n",
    "* Temos várias maneiras de acessar, inicializar e vincular os parâmetros do modelo.\n",
    "* Podemos usar inicialização personalizada.\n",
    "\n",
    "\n",
    "## Exercícios\n",
    "\n",
    "1. Use o modelo `FancyMLP` definido em :numref:`sec_model_construction` e acesse os parâmetros das várias camadas.\n",
    "1. Observe o documento do módulo de inicialização para explorar diferentes inicializadores.\n",
    "1. Construa um MLP contendo uma camada de parâmetros compartilhados e treine-o. Durante o processo de treinamento, observe os parâmetros do modelo e gradientes de cada camada.\n",
    "1. Por que compartilhar parâmetros é uma boa ideia?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 71,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "[Discussão](https://discuss.d2l.ai/t/269)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 72
   },
   "source": [
    "<!--stackedit_data:\n",
    "eyJoaXN0b3J5IjpbMjI3MjUyNjYwLC03MTk0NTg0ODEsMjUwMT\n",
    "g4NTk3LC0xNTkzNzg3NzI4LDE2NDI5Nzg1MDFdfQ==\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}